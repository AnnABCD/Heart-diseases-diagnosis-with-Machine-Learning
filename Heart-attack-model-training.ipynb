{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8331f",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "categorical_features = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "quantitative_features = ['age','trtbps','chol','thalachh','oldpeak']\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "# Pre-processing function : clean data, fill missing values and encode categorical data\n",
    "def datapreprocessing(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            scaler = StandardScaler()\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features : \n",
    "          labelencoder=LabelEncoder()\n",
    "          data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    Y = data.loc[:,'output']\n",
    "    X = data.drop(['output'],axis=1) \n",
    "    \n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455afd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing datset\n",
    "datacopy = data.copy()\n",
    "X, Y = datapreprocessing(datacopy) \n",
    "\n",
    "# Splitting traing dataset for validation testing (70% for training and 30% for validation)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937223b",
   "metadata": {},
   "source": [
    "# PART 4 : MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab49e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection : supervised classification methods\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "#models.append(('XGb', XGBClassifier()))\n",
    "\n",
    "def model_comparison(models, x, y):\n",
    "      \n",
    "    names = []\n",
    "    results = []\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)  \n",
    "    for name, model in models:\n",
    "        cv_results = cross_val_score(model, x, y, cv=kfold, scoring='accuracy')\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        print('%s Cross validation accuracy: %f (SD = %f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "            \n",
    "    # Visualization     \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(results, labels = names)\n",
    "    plt.title('Models accuracy comparison')\n",
    "    plt.ylabel('Model Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e90084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_comparison(models, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db897a",
   "metadata": {},
   "source": [
    "# PART 5 : MODEL OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2364b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for optimization using dictionaries {parameter name: parameter list}\n",
    "LR_params = {'C':[0.1, 0.5, 1, 10]}\n",
    "SVM_params = {'C':[0.01, 0.1, 1, 10], 'kernel':['rbf' ,'linear', 'poly', 'sigmoid']}\n",
    "RF_params = {'n_estimators':[10,50,100]}\n",
    "\n",
    "# Append list of models with parameter dictionaries\n",
    "models_opt = []\n",
    "models_opt.append(('LogisticRegression', LogisticRegression(), LR_params))\n",
    "models_opt.append(('SVM', SVC(), SVM_params))\n",
    "models_opt.append(('RandomForest',  RandomForestClassifier(), RF_params))\n",
    "\n",
    "def model_optimization(models, x, y):\n",
    "    \n",
    "    names = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    best_estimators = []\n",
    "    \n",
    "    # Gridsearch method for model optimization \n",
    "    for name, model, params in models:    \n",
    "       \n",
    "        model_grid = GridSearchCV(model, params, scoring='accuracy')\n",
    "        model_grid = model_grid.fit(x, y)\n",
    "        accur=model_grid.best_score_\n",
    "        \n",
    "        model_grid = GridSearchCV(model, params, scoring='f1')\n",
    "        model_grid = model_grid.fit(x, y)\n",
    "        f1=model_grid.best_score_\n",
    "        \n",
    "        names.append(name) \n",
    "        best_estimators.append(model_grid.best_estimator_)\n",
    "        accuracy_scores.append(accur)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "        print(\"Cross Validation %s : Accuracy = %f / F1score = %f\" % (name, accur, f1))\n",
    "        \n",
    "    # Scores bar plot\n",
    "    x = np.arange(len(names))\n",
    "    width = 0.1 \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.bar(x - width, accuracy_scores, 2*width, label='accuracy')\n",
    "    ax.bar(x + width, f1_scores, 2*width, label='f1')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Algorithms performance (Training set)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.legend(loc ='lower right')\n",
    "    \n",
    "    return(best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804969c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = model_optimization(models_opt, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706d0a4",
   "metadata": {},
   "source": [
    "# PART 6 : TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bfb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeltesting(models):\n",
    "    \n",
    "    test_accuracy_scores = []\n",
    "    test_f1_scores = []\n",
    "    test_precision_scores = []\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=(30,10))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        precision = precision_score(Y_test, Y_pred)\n",
    "        f1 = f1_score(Y_test, Y_pred)  \n",
    "\n",
    "        print('%s: accuracy %f precision %f f1 %f' % (model, accuracy, precision, f1))\n",
    "        \n",
    "        confusion = confusion_matrix(Y_test, Y_pred)\n",
    "        ConfusionMatrixDisplay(confusion).plot(ax=axs[i])\n",
    "        axs[i].set_title('%s: Confusion matrix' % (model))   \n",
    "        \n",
    "        test_accuracy_scores.append(accuracy)\n",
    "        test_f1_scores.append(f1)\n",
    "        test_precision_scores.append(precision)\n",
    "            \n",
    "    return(test_accuracy_scores, test_f1_scores, test_precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_f1, test_precision = modeltesting(best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores bar plot\n",
    "labels = ['Logistic Regression', 'SVM', 'RandomForest']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.bar(x - width, test_acc, width, label='accuracy')\n",
    "ax.bar(x, test_f1, width, label='f1')\n",
    "ax.bar(x + width, test_f1, width, label='precision')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Algorithms performance (Testing set)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc ='lower right')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
